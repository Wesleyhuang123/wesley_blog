[
  {
    "objectID": "Tasks.html",
    "href": "Tasks.html",
    "title": "Tasks",
    "section": "",
    "text": "1+1\n\n[1] 2\n\n\n\n90/30\n\n[1] 3\n\n\n\nPut numbers into variables, do simple math on the variables\n\nz=5\nb=4\nz+b\n\n[1] 9\n\n\nWrite code that will place the numbers 1 to 100 separately into a variable using for loop. Then, again using the seq function.\n\na <- 1:100\na\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\n\n4 . Find the sum of all integers from 1 to 100.\n\n1:100\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\nsum(1:100)\n\n[1] 5050\n\n\n\nWrite a function to find the sum of all integers between any two values.\n\nmy_sum_range <- function(min,max){\n  my_numbers <- min:max\n  the_sum <- 0\n  for( i in my_numbers){\n    the_sum <- the_sum+ i\n  }\n  return(the_sum)\n\n  }\nmy_sum_range(min=10, max=30)\n\n[1] 420\n\n\nList all of the odd numbers from 1 to 100.\n\n\nseq(1,100,2)\n\n [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49\n[26] 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99\n\n\n\nGenerate 100 random numbers\n\n  for (i in 101) {\n\n  }\n\n\nFizzBuzz Problem\n\n  for (i in 1:100){\n    if(i%%3==0){\n      print(\"Fizz\")\n    if(i%%5==0){\n      print(\"Buzz\")\n      }\n    }\n  }\n\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n\n\n\nFizzBuzz Problem\n\nfor (i in 1:100)\n  if(i%%3==0 && i%%5==0){\n    print(\"FizzBuzz\")\n  } else if(i%%5==0){\n    print(\"Buzz\")\n  } else if(i%%3==0){\n    print(\"Fizz\")\n    }\n\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"FizzBuzz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"FizzBuzz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"FizzBuzz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"FizzBuzz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"FizzBuzz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"FizzBuzz\"\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] \"Fizz\"\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/second post/index.html",
    "href": "posts/second post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "Week 3: Directed Forgetting Paper Thoughts\nIt was interesting to know that through each experiments the researchers were able to produce results that indicate that a directed forgetting reduces memory for perceptual details of pictures. I liked how each experiment, the researchers were trying to build on possible errors to disprove in the previous experiments. For example, adding the control condition in experiment 3 where participants were simply told to remember all the study items for a later recognition test. Doing so can more accurately test the item-based directed forgetting because R-cued items in the DF experimental condition would be greater than memory performance for the items in the remember-all control condition.\nIn my opinion, I think that the exemplar distractors produced their results because the images were very similar to the original ones from their categories. This probably led to the longer response times and the less portion of accuracy that were correct compared to the Novel Test. I also think that a key detail to note is that the researchers took different participants each time for each experiment. Each participant may have had a different skill set that could have affect the results, however by choosing different participants we can guarantee that each different experiment had results based on participants who were taking the memory tests fresh each time and not getting better from experiment to experiment.\nThoughts and Feedback on New Experiment: The images were flashing too quickly in the beginning that there was no time to pick up on perceptual details of an image. It was difficult to purposely remember/forget certain images. Additionally, at a specific time during the encoding phase (towards the middle and the end), it was difficult to keep my attention span on the test. At a certain point it felt tedious and it was difficult to make the decision to purposely remember or forget certain images. However, I felt like the exemplar distractor did make me have to think more before selecting an image during the memory test, where during the novel test without the distractors it was more easy for me to select the obvious image I saw previously. Overall, I think that the attention span is the most difficult to overcome to make the results of the experiment as accurate as possible"
  },
  {
    "objectID": "posts/Seventh Post/index.html",
    "href": "posts/Seventh Post/index.html",
    "title": "Seventh Blog Post",
    "section": "",
    "text": "This week I learned about loading data onto R Studio and different ways to analyze the data. I practiced with the different coding functions on my tasks slide.\nMy goals for this week are:\n\nRead through the sources supplied to me and get a gist of data analysis on R Studio.\nTo successfully install and display data from tidyverse and load it onto R Studio.\nAnalyze the installed data using different coding functions from the resources Professor Crump supplied to me.\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nstarwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#   vehicles <list>, starships <list>, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       <chr> \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     <int> 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       <dbl> 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color <chr> \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color <chr> \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  <chr> \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year <dbl> 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        <chr> \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     <chr> \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  <chr> \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    <chr> \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      <list> <\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   <list> <\"Snowspeeder\", \"Imperial Speeder Bike\">, <>, <>, <>, \"Imp…\n$ starships  <list> <\"X-wing\", \"Imperial shuttle\">, <>, <>, \"TIE Advanced x1\",…\n\n?starwars\nview(starwars)\n\nncol(starwars)\n\n[1] 14\n\nnrow(starwars)\n\n[1] 87\n\ndim(starwars)\n\n[1] 87 14\n\ndim(starwars)[1]\n\n[1] 87\n\ndim(starwars)[2]\n\n[1] 14\n\nggplot(data=starwars, aes(x=height, y=mass))+ geom_point()\n\nWarning: Removed 28 rows containing missing values (geom_point).\n\n\n\n\nggplot(data=starwars, aes(x=height, y=birth_year))+ geom_point()\n\nWarning: Removed 44 rows containing missing values (geom_point)."
  },
  {
    "objectID": "posts/Third Post/index.html",
    "href": "posts/Third Post/index.html",
    "title": "Second Blog Post",
    "section": "",
    "text": "Week 2: Coding\nThis week I was able to figure out how to code problems 1,2, and 4 on my own. I found problems 3 to 5 a bit challenging but after watching Professor Crump’s videos I was able to figure it out. Professor Crump does a great job at breaking down each function and explaining how the code is executed. However, problem 6 seemed to be the most challenging for me and I got lost trying to follow the code executed.\nAnother problem I ran into was trying to change my image on my second blog post to a photo I found from google. It was a jpeg file and I am not entirely aware of how to do that. I tried images/(name).jpeg but it did not work. Hopefully, I can figure this out with my next meeting with the Professor.\nMy goals for next week are to continue with the practice problems."
  },
  {
    "objectID": "posts/Fifth Post/index.html",
    "href": "posts/Fifth Post/index.html",
    "title": "Fifth Blog Post",
    "section": "",
    "text": "Through the early history (Brown (1954), Muther (1965), Bjork (1972) ) of the memory experiments, the name “directed forgetting” was agreed upon by researchers for future experiments and data added onto the research. I wonder how this name came to be and what was the purpose behind the directed forgetting experiments. What results were we looking for and how did these results help us better understand cognition or improve the real world? I found it particularly interesting that Muther (1965) was able to find results that supported better recall on a list where 12 words were to be remembered, and 12 words were to be forgotten than a list where all 24 words had to be remembered. He explained these results to be due to a lowered attentional demand, reduced proactive interference, more focused rehearsal, and easier organization. We can relate this to the data we received in our novel/exemplar directed forgetting test for the first 6 participants. One important thing to note is that through the history of these experiments, the researchers often built upon research already done in the past without making big changes. This is something we did in our current experiment. Another important thing to note is that David Helms (1969) found that the F cue must be distinctive and explicit usage of the word “forget” appears to make a difference. This was done in our revised experiment from the original Patrick study where we only used F and R keys, now we explicitly use the words “Forget” or “Remember” and have a rating system in place that we can attribute to a better/more accurate results of directed forgetting. Initially, I thought that recognition would be better than recall since the act of recognizing something takes less cognitive skills than recalling something from your memory. However, in the paper studies have shown the opposite. Recall was found to be often more accurate than recognition. This is something that sparks interest in me and I wonder why this is the case."
  },
  {
    "objectID": "posts/first-post/index.html",
    "href": "posts/first-post/index.html",
    "title": "First Blog Post",
    "section": "",
    "text": "The concept of “R” language appears to be quite difficult starting from scratch and conquering it in a short amount of time seems to be a task out of reality. Despite all of this, I am eager to get to practice and improve in the craft of coding. By the end of this program, I hope that I will be well versed in learning more about coding from my mentor Professor Matt Crump and my peer Drew. I am excited about the upcoming tasks this semester and to seek the results from the experiments we will be conducting.\nSome issues I have ran into my first week of learning to blog on RStudio are:\n\nHow to create a new post with my own images (Personal or from the Internet)\nHow to properly get my changes from RStudio onto github\n\nSome goals I have for next week:\n\nI plan to personalize my own blog with either my own images or with some form of creativity that I can come up with.\nI plan to learn how to efficiently create new posts and get my changes onto RStudio.\nI look forward to the next steps in the research process and tasks my mentor may provide me to learn more about coding."
  },
  {
    "objectID": "posts/Fourth Post/index.html",
    "href": "posts/Fourth Post/index.html",
    "title": "Fourth Blog Post",
    "section": "",
    "text": "Upon coding and gathering information on the experiment, the first six participants were able to produce these results:\nOur Data:\n\nWe were able to reproduce results similar to the original Ahmad (2019) experiment regarding directed forgetting. As indicated in the graph above for 500ms and 2000ms, we were able to find directed forgetting for both the exemplar and novel tests. Additionally, as expected people were more likely to get more correct and “remember” more during the novel test than the exemplar test because simply the novel test was designed to be easier to recognize previously seen images. However, the results of the exemplar test in the 2000ms appears to be considerably dramatic and the proportion correct between the “Forgetting” and “Remembering” in the exemplar test should close up as we get more results.\nThis is a good sign. Going forward, if we are able to continue to acquire similar results as we collect more data, then we can safely say that our changes to this experiment from the one that Patrick did were helpful in attempting to replicate the original results. We expect to collect results from 50 people."
  },
  {
    "objectID": "posts/Eighth Post/index.html",
    "href": "posts/Eighth Post/index.html",
    "title": "Eighth Blog Post",
    "section": "",
    "text": "Week 8: Data Analysis"
  },
  {
    "objectID": "posts/Sixth Post/index.html",
    "href": "posts/Sixth Post/index.html",
    "title": "Sixth Blog Post",
    "section": "",
    "text": "This week I learned about creating a slides tab on my blog website and the numerous of other tools available to help me in editing those slides. I discussed with Professor Crump about the best approaches for me to take in creating a nice presentation and how to execute it efficiently on the day of the conference in December.\nMy goals for this week are:\n\nTo create a format I want for my slides (Introduction, Research Question, Why It Matters, Research Method, Conclusion…etc)\nPlay around with reveal js functions on R Studio and experiment different styles I can use to make my presentation more interesting."
  },
  {
    "objectID": "Slides.html#introduction",
    "href": "Slides.html#introduction",
    "title": "Slides",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Wesley and today I will be presenting my research on directed forgetting."
  },
  {
    "objectID": "Slides.html#research-questionwhy-it-matters",
    "href": "Slides.html#research-questionwhy-it-matters",
    "title": "Slides",
    "section": "Research Question/Why It Matters",
    "text": "Research Question/Why It Matters"
  },
  {
    "objectID": "Slides.html#research-method",
    "href": "Slides.html#research-method",
    "title": "Slides",
    "section": "Research Method",
    "text": "Research Method"
  },
  {
    "objectID": "Slides.html#results",
    "href": "Slides.html#results",
    "title": "Slides",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "Slides.html#conclusion",
    "href": "Slides.html#conclusion",
    "title": "Slides",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wesley_blog",
    "section": "",
    "text": "r\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nWesley Huang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nlearn\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\nWesley Huang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "E3_DF_pairs.html",
    "href": "E3_DF_pairs.html",
    "title": "E3_DF_paired",
    "section": "",
    "text": "Data collected 7/11/22"
  },
  {
    "objectID": "E3_DF_pairs.html#import-data",
    "href": "E3_DF_pairs.html#import-data",
    "title": "E3_DF_paired",
    "section": "Import Data",
    "text": "Import Data\n\n# Read the text file from JATOS ...\nread_file('data/E3/jatos_results_20221103143242.txt') %>%\n  # ... split it into lines ...\n  str_split('\\n') %>% first() %>%\n  # ... filter empty rows ...\n  discard(function(x) x == '') %>%\n  # ... parse JSON into a data.frame\n  map_dfr(fromJSON, flatten=T) -> all_data"
  },
  {
    "objectID": "E3_DF_pairs.html#demographics",
    "href": "E3_DF_pairs.html#demographics",
    "title": "E3_DF_paired",
    "section": "Demographics",
    "text": "Demographics\n\nlibrary(tidyr)\n\ndemographics <- all_data %>%\n  filter(trial_type == \"survey-html-form\") %>%\n  select(ID,response) %>%\n  unnest_wider(response) %>%\n  mutate(age = as.numeric(age))\n\nage_demographics <- demographics %>%\n  summarize(mean_age = mean(age),\n            sd_age = sd(age),\n            min_age = min(age),\n            max_age = max(age))\n\nfactor_demographics <- apply(demographics[-1], 2, table)\n\nA total of 17 participants were recruited from Amazon’s Mechanical Turk. Mean age was 19.1 (range = 18 to 22 ). There were 9 females, and 8 males. There were 15 right-handed participants, and NA left or both handed participants. 10 participants reported normal vision, and 5 participants reported corrected-to-normal vision. 14 participants reported English as a first language, and 3 participants reported English as a second language."
  },
  {
    "objectID": "E3_DF_pairs.html#pre-processing",
    "href": "E3_DF_pairs.html#pre-processing",
    "title": "E3_DF_paired",
    "section": "Pre-processing",
    "text": "Pre-processing\nWe are interested in including participants who attempted to perform the task to the best of their ability. We adopted the following exclusion criteria.\n\nLower than 75% correct during the encoding task. This means that participants failed to correctly press the F or R keys on each trial.\n\n\n# select data from the study phase\n# study_accuracy <- all_data %>%\n#   filter(experiment_phase == \"study\",\n#          is.na(correct) == FALSE) %>%\n#   group_by(ID)%>%\n#   summarize(mean_correct = mean(correct))\n\nstudy_accuracy <- all_data %>%\n  filter(experiment_phase == \"study\",\n         trial_type == \"html-slider-response\",\n         is.na(rt) == FALSE) %>%\n  mutate(response = as.numeric(unlist(response))) %>%\n  mutate(correct = case_when(encoding_instruction == \"R\" && response > 50 ~ TRUE,\n                             encoding_instruction == \"R\" && response < 50 ~ FALSE,\n                             encoding_instruction == \"F\" && response > 50 ~ FALSE,\n                             encoding_instruction == \"F\" && response < 50 ~ TRUE)) %>%\n  filter(is.na(correct) == FALSE) %>%\n  group_by(ID)%>%\n  summarize(mean_correct = mean(correct))\n\nstudy_excluded_subjects <- study_accuracy %>%\n  filter(mean_correct < .75) %>%\n  pull(ID)\n\nggplot(study_accuracy, aes(x=mean_correct))+\n  coord_cartesian(xlim=c(0,1))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean correct responses \\n for each subject during study phase\")\n\n\n\n\n\nMore than 25% Null responses (120*.25 = 30) during test. NULL responses mean that the participant did not respond on a test trial after 10 seconds.\n\n\n# select data from the study phase\ntest_null <- all_data %>%\n  filter(experiment_phase == \"test\"),\n         response ==\"NULL\") %>%\n  group_by(ID) %>%\n  count()\n\ntest_null_excluded <- test_null %>%\n  filter(n > (120*.25)) %>%\n  pull(ID)\n\nggplot(test_null, aes(x=n))+\n  geom_vline(xintercept=30)+\n  geom_histogram()+\n  ggtitle(\"Histogram of count of null responses \\n for each subject during test\")\n\n\nHigher than 75% response bias in the recognition task. This suggests that participants were simply pressing the same button on most trials.\n\n\ntest_response_bias <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\") %>%\n  mutate(response = as.numeric(response)) %>%\n  group_by(ID, response) %>%\n  count() %>%\n  pivot_wider(names_from = response,\n              values_from = n,\n              values_fill = 0) %>%\n  mutate(bias = abs(`0` - `1`)/120)\n\ntest_response_bias_excluded <- test_response_bias %>%\n  filter(bias > .75) %>%\n  pull(ID)\n\nggplot(test_response_bias, aes(x=bias))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nMaking responses too fast during the recognition memory test, indicating that they weren’t performing the task. We excluded participants whose mean RT was less than 300 ms.\n\n\ntest_mean_rt <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\",\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt)) %>%\n  group_by(ID) %>%\n  summarize(mean_RT = mean(rt))\n\ntest_mean_rt_excluded <- test_mean_rt %>%\n  filter(mean_RT < 300) %>%\n  pull(ID)\n\nggplot(test_mean_rt, aes(x=mean_RT))+\n  geom_vline(xintercept=300)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nSubjects are included if they perform better than 55% correct on the novel lures.\n\n\ntest_mean_novel_accuracy <- all_data %>%\n  filter(experiment_phase == \"test\",\n         test_condition == \"novel\") %>%\n  mutate(correct = as.logical(correct)) %>%\n  group_by(ID) %>%\n  summarize(mean_correct = mean(correct))\n\ntest_mean_novel_accuracy_excluded <- test_mean_novel_accuracy %>%\n  filter(mean_correct < .4) %>%\n  pull(ID)\n\nggplot(test_mean_novel_accuracy, aes(x=mean_correct))+\n  geom_vline(xintercept=.4)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean accuracy for novel lures \\n for each subject during test phase\")"
  },
  {
    "objectID": "E3_DF_pairs.html#all-exclusions",
    "href": "E3_DF_pairs.html#all-exclusions",
    "title": "E3_DF_paired",
    "section": "All exclusions",
    "text": "All exclusions\n\nall_excluded <- unique(c(study_excluded_subjects,\n                  test_response_bias_excluded,\n                  test_mean_rt_excluded,\n                  test_mean_novel_accuracy_excluded))\n\nlength(all_excluded)\n\n[1] 0\n\n\nOur participants were recruited online and completed the experiment from a web browser. Our experiment script requests that participants attempt the task to the best of their ability. Nevertheless, it is possible that participants complete the experiment and submit data without attempting to complete the task as directed. We developed a set of criteria to exclude participants whose performance indicated they were not attempting the task as instructed. These criteria also allowed us to confirm that the participants we included in the analysis did attempt the task as instructed to the best of their ability. We adopted the following five criteria:\nFirst, during the encoding phase participants responded to each instructional cue (to remember or forget the picture on each trial) by pressing “R” or “F” on the keyboard. This task demand further served as an attentional check. We excluded participants who scored lower than 75% correct on instructional cue identification responses. Second, participants who did not respond on more than 25% of trials in the recognition test were excluded. Third, we measured response bias (choosing the left or right picture) during the recognition test, and excluded participants who made 75% of their responses to one side (indicating they were repeatedly pressing the same button on each trial). Fourth, we excluded participants whose mean reaction time during the recognition test was less than 300ms, indicating they were pressing the buttons as fast as possible without making a recognition decision. Finally, we computed mean accuracy for the novel lure condition for all participants, and excluded participants whose mean accuracy was less than 55% for those items. All together 0 participants were excluded."
  },
  {
    "objectID": "E3_DF_pairs.html#define-helper-functions",
    "href": "E3_DF_pairs.html#define-helper-functions",
    "title": "E3_DF_paired",
    "section": "Define Helper functions",
    "text": "Define Helper functions\nTo do, consider moving the functions into the R package for this project\n\n# attempt general solution\n\n## Declare helper functions\n\n################\n# get_mean_sem\n# data = a data frame\n# grouping_vars = a character vector of factors for analysis contained in data\n# dv = a string indicated the dependent variable colunmn name in data\n# returns data frame with grouping variables, and mean_{dv}, sem_{dv}\n# note: dv in mean_{dv} and sem_{dv} is renamed to the string in dv\n\nget_mean_sem <- function(data, grouping_vars, dv, digits=3){\n  a <- data %>%\n    group_by_at(grouping_vars) %>%\n    summarize(\"mean_{ dv }\" := round(mean(.data[[dv]]), digits),\n              \"sem_{ dv }\" := round(sd(.data[[dv]])/sqrt(length(.data[[dv]])),digits),\n              .groups=\"drop\")\n  return(a)\n}\n\n################\n# get_effect_names\n# grouping_vars = a character vector of factors for analysis\n# returns a named list\n# list contains all main effects and interaction terms\n# useful for iterating the computation means across design effects and interactions\n\nget_effect_names <- function(grouping_vars){\n  effect_names <- grouping_vars\n  if( length(grouping_vars > 1) ){\n    for( i in 2:length(grouping_vars) ){\n      effect_names <- c(effect_names,apply(combn(grouping_vars,i),2,paste0,collapse=\":\"))\n    }\n  }\n  effects <- strsplit(effect_names, split=\":\")\n  names(effects) <- effect_names\n  return(effects)\n}\n\n################\n# print_list_of_tables\n# table_list = a list of named tables\n# each table is printed \n# names are header level 3\n\nprint_list_of_tables <- function(table_list){\n  for(i in 1:length(table_list)){\n    cat(\"###\",names(table_list[i]))\n    cat(\"\\n\")\n    print(knitr::kable(table_list[[i]]))\n    cat(\"\\n\")\n  }\n}"
  },
  {
    "objectID": "E3_DF_pairs.html#conduct-analysis",
    "href": "E3_DF_pairs.html#conduct-analysis",
    "title": "E3_DF_paired",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nAccuracy <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nAccuracy$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE)\n\n# declare factors, IVS, subject variable, and DV\nAccuracy$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nAccuracy$factors$subject <- \"ID\"\nAccuracy$factors$DV <- \"correct\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nAccuracy$subject_means <- get_mean_sem(data=Accuracy$filtered_data,\n                                       grouping_vars = c(Accuracy$factors$subject,\n                                                         Accuracy$factors$IVs),\n                                       dv = Accuracy$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nAccuracy$effects <- get_effect_names(Accuracy$factors$IVs)\n\nAccuracy$means <- lapply(Accuracy$effects, FUN = function(x) {\n  get_mean_sem(data=Accuracy$filtered_data,\n             grouping_vars = x,\n             dv = Accuracy$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nAccuracy$subject_means <- Accuracy$subject_means %>%\n  mutate_at(Accuracy$factors$IVs,factor) %>%\n  mutate_at(Accuracy$factors$subject,factor)\n\n# run ANOVA\nAccuracy$aov.out <- aov(mean_correct ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), Accuracy$subject_means)\n\n# save printable summaries\nAccuracy$apa_print <- papaja::apa_print(Accuracy$aov.out)"
  },
  {
    "objectID": "E3_DF_pairs.html#graphs",
    "href": "E3_DF_pairs.html#graphs",
    "title": "E3_DF_paired",
    "section": "Graphs",
    "text": "Graphs\n\nAccuracy$graphs$figure <- ggplot(Accuracy$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_correct,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_correct-sem_correct,\n                    ymax = mean_correct+sem_correct),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(.4,1))+\n  geom_hline(yintercept=.5)+\n  scale_y_continuous(breaks = seq(0.4,1,.1))+\n  theme_classic(base_size=12)+\n  ylab(\"Proportion Correct\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E2: Proportion Correct by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nAccuracy$graphs$figure"
  },
  {
    "objectID": "E3_DF_pairs.html#print-anova",
    "href": "E3_DF_pairs.html#print-anova",
    "title": "E3_DF_paired",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(Accuracy$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n16\n3.1130602\n0.1945663\nNA\nNA\n\n\nencoding_stimulus_time\n2\n0.0091935\n0.0045967\n0.2608783\n0.7719972\n\n\nResiduals\n32\n0.5638460\n0.0176202\nNA\nNA\n\n\nencoding_instruction\n1\n0.0160771\n0.0160771\n1.0135683\n0.3290333\n\n\nResiduals\n16\n0.2537895\n0.0158618\nNA\nNA\n\n\ntest_condition\n1\n1.6157060\n1.6157060\n32.8642299\n0.0000308\n\n\nResiduals\n16\n0.7866089\n0.0491631\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n0.0262066\n0.0131033\n0.8625309\n0.4316737\n\n\nResiduals\n32\n0.4861345\n0.0151917\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n0.0240190\n0.0120095\n0.8095293\n0.4539739\n\n\nResiduals\n32\n0.4747249\n0.0148352\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n0.0176031\n0.0176031\n0.5505816\n0.4688335\n\n\nResiduals\n16\n0.5115482\n0.0319718\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n0.1241252\n0.0620626\n3.1030417\n0.0586515\n\n\nResiduals\n32\n0.6400183\n0.0200006\nNA\nNA"
  },
  {
    "objectID": "E3_DF_pairs.html#print-means",
    "href": "E3_DF_pairs.html#print-means",
    "title": "E3_DF_paired",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(Accuracy$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_correct\nsem_correct\n\n\n\n\n500\n0.716\n0.017\n\n\n1000\n0.707\n0.017\n\n\n2000\n0.737\n0.017\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\nF\n0.715\n0.014\n\n\nR\n0.725\n0.014\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nexemplar\n0.633\n0.015\n\n\nnovel\n0.807\n0.012\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\n500\nF\n0.703\n0.025\n\n\n500\nR\n0.729\n0.024\n\n\n1000\nF\n0.715\n0.025\n\n\n1000\nR\n0.700\n0.025\n\n\n2000\nF\n0.726\n0.024\n\n\n2000\nR\n0.747\n0.024\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nexemplar\n0.626\n0.026\n\n\n500\nnovel\n0.806\n0.021\n\n\n1000\nexemplar\n0.632\n0.026\n\n\n1000\nnovel\n0.782\n0.022\n\n\n2000\nexemplar\n0.641\n0.026\n\n\n2000\nnovel\n0.832\n0.020\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nF\nexemplar\n0.613\n0.022\n\n\nF\nnovel\n0.810\n0.017\n\n\nR\nexemplar\n0.652\n0.021\n\n\nR\nnovel\n0.804\n0.018\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nF\nexemplar\n0.578\n0.038\n\n\n500\nF\nnovel\n0.822\n0.029\n\n\n500\nR\nexemplar\n0.672\n0.036\n\n\n500\nR\nnovel\n0.789\n0.032\n\n\n1000\nF\nexemplar\n0.661\n0.037\n\n\n1000\nF\nnovel\n0.767\n0.032\n\n\n1000\nR\nexemplar\n0.605\n0.037\n\n\n1000\nR\nnovel\n0.798\n0.031\n\n\n2000\nF\nexemplar\n0.600\n0.039\n\n\n2000\nF\nnovel\n0.839\n0.027\n\n\n2000\nR\nexemplar\n0.678\n0.035\n\n\n2000\nR\nnovel\n0.825\n0.030"
  },
  {
    "objectID": "E3_DF_pairs.html#comparisons",
    "href": "E3_DF_pairs.html#comparisons",
    "title": "E3_DF_paired",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nAccuracy$simple$DF_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nAccuracy$simple$test_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "E3_DF_pairs.html#write-up",
    "href": "E3_DF_pairs.html#write-up",
    "title": "E3_DF_paired",
    "section": "Write-up",
    "text": "Write-up\n\n## helper print functions\nqprint <- function(data,iv,level,dv){\n   data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv)\n}\n\nqprint_mean_sem <- function(data,iv,level,dv){\n   dv_mean <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[1])\n   \n   dv_sem <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[2])\n   \n   return(paste(\"M = \", \n    dv_mean,\n    \", SEM = \",\n    dv_sem,\n    sep=\"\"))\n   \n}\n\n# qprint(Accuracy$means,\"encoding_stimulus_time\",\"500\",\"mean_correct\")\n# qprint_mean_sem(Accuracy$means,\"encoding_stimulus_time\",\"500\",c(\"mean_correct\",\"sem_correct\"))\n\n# use data.table for interactions\n\n#t <- as.data.table(Accuracy$means$`encoding_stimulus_time:encoding_instruction`)\n#t[encoding_stimulus_time==500 & encoding_instruction == \"F\"]$mean_correct\n\nProportion correct for each subject in each condition was submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For completeness, each main effect and higher-order interaction is described in turn.\nThe main effect of encoding duration was significant, \\(F(2, 32) = 0.26\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .772\\), \\(\\hat{\\eta}^2_G = .001\\). Proportion correct was lowest for the 500 ms duration (M = 0.716, SEM = 0.017), and higher for the 1000 ms (M = 0.707, SEM = 0.017), and 2000 ms (M = 0.737, SEM = 0.017) stimulus durations.\nThe main effect of encoding instruction was not significant, \\(F(1, 16) = 1.01\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .329\\), \\(\\hat{\\eta}^2_G = .002\\). Proportion correct was similar for remember cues (M = 0.725, SEM = 0.014) and forget cues (M = 0.715, SEM = 0.014).\nThe main effect of lure type was significant, \\(F(1, 16) = 32.86\\), \\(\\mathit{MSE} = 0.05\\), \\(p < .001\\), \\(\\hat{\\eta}^2_G = .191\\). Proportion correct was higher for novel lures (M = 0.807, SEM = 0.012) than exemplar lures (M = 0.633, SEM = 0.015).\nThe main question of interest was whether directing forgetting would vary across the encoding duration times. The interaction between encoding instruction and encoding duration was not significant, \\(F(2, 32) = 0.86\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .432\\), \\(\\hat{\\eta}^2_G = .004\\).\nPaired sample t-tests were used to assess the directed forgetting effect at each encoding duration. The directed forgetting effect is taken as the difference between proportion correct for remember minus forget items. At 500 ms, the directed forgetting effect was not significant, \\(M = 0.03\\), 95% CI \\([-0.03, 0.09]\\), \\(t(16) = 1.05\\), \\(p = .309\\). At 1000ms, the directed forgetting effect was not significant, \\(M = -0.01\\), 95% CI \\([-0.07, 0.04]\\), \\(t(16) = -0.51\\), \\(p = .616\\). And, at 2000 ms, the directed forgetting effect was again not detected, \\(M = 0.04\\), 95% CI \\([-0.03, 0.11]\\), \\(t(16) = 1.11\\), \\(p = .285\\).\nThe encoding duration by lure type interaction was not significnat, \\(F(2, 32) = 0.81\\), \\(\\mathit{MSE} = 0.01\\), \\(p = .454\\), \\(\\hat{\\eta}^2_G = .004\\). The encoding instruction by lure type interaction was not significant, \\(F(1, 16) = 0.55\\), \\(\\mathit{MSE} = 0.03\\), \\(p = .469\\), \\(\\hat{\\eta}^2_G = .003\\). Similarly, the interaction between encoding duration, instruction, and lure type was not significant, \\(F(2, 32) = 3.10\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .059\\), \\(\\hat{\\eta}^2_G = .018\\)."
  },
  {
    "objectID": "E3_DF_pairs.html#conduct-analysis-1",
    "href": "E3_DF_pairs.html#conduct-analysis-1",
    "title": "E3_DF_paired",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nRT <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nRT$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE,\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt)) %>%\n  filter(rt < 10000)\n\n# declare factors, IVS, subject variable, and DV\nRT$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nRT$factors$subject <- \"ID\"\nRT$factors$DV <- \"rt\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nRT$subject_means <- get_mean_sem(data=RT$filtered_data,\n                                       grouping_vars = c(RT$factors$subject,\n                                                         RT$factors$IVs),\n                                       dv = RT$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nRT$effects <- get_effect_names(RT$factors$IVs)\n\nRT$means <- lapply(RT$effects, FUN = function(x) {\n  get_mean_sem(data=RT$filtered_data,\n             grouping_vars = x,\n             dv = RT$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nRT$subject_means <- RT$subject_means %>%\n  mutate_at(RT$factors$IVs,factor) %>%\n  mutate_at(RT$factors$subject,factor)\n\n# run ANOVA\nRT$aov.out <- aov(mean_rt ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), RT$subject_means)\n\n# save printable summaries\nRT$apa_print <- papaja::apa_print(RT$aov.out)"
  },
  {
    "objectID": "E3_DF_pairs.html#graphs-1",
    "href": "E3_DF_pairs.html#graphs-1",
    "title": "E3_DF_paired",
    "section": "Graphs",
    "text": "Graphs\n\nRT$graphs$figure <- ggplot(RT$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_rt,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_rt-sem_rt,\n                    ymax = mean_rt+sem_rt),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(1500,2500))+\n  scale_y_continuous(breaks = seq(1500,2500,100))+\n  theme_classic(base_size=12)+\n  ylab(\"Mean RT (ms)\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E2: Mean RT by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nRT$graphs$figure"
  },
  {
    "objectID": "E3_DF_pairs.html#print-anova-1",
    "href": "E3_DF_pairs.html#print-anova-1",
    "title": "E3_DF_paired",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(RT$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n16\n49350162.10\n3084385.13\nNA\nNA\n\n\nencoding_stimulus_time\n2\n72701.12\n36350.56\n0.3413044\n0.7133985\n\n\nResiduals\n32\n3408153.64\n106504.80\nNA\nNA\n\n\nencoding_instruction\n1\n382264.12\n382264.12\n4.5671308\n0.0483789\n\n\nResiduals\n16\n1339183.45\n83698.97\nNA\nNA\n\n\ntest_condition\n1\n3004083.08\n3004083.08\n26.2757694\n0.0001016\n\n\nResiduals\n16\n1829264.39\n114329.02\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n123407.60\n61703.80\n0.7350722\n0.4873919\n\n\nResiduals\n32\n2686160.23\n83942.51\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n45867.03\n22933.52\n0.1314855\n0.8772632\n\n\nResiduals\n32\n5581394.99\n174418.59\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n304516.82\n304516.82\n2.1876809\n0.1585367\n\n\nResiduals\n16\n2227138.88\n139196.18\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n105224.63\n52612.31\n0.5913009\n0.5595420\n\n\nResiduals\n32\n2847271.24\n88977.23\nNA\nNA"
  },
  {
    "objectID": "E3_DF_pairs.html#print-means-1",
    "href": "E3_DF_pairs.html#print-means-1",
    "title": "E3_DF_paired",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(RT$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_rt\nsem_rt\n\n\n\n\n500\n1805.934\n43.652\n\n\n1000\n1778.400\n50.593\n\n\n2000\n1816.666\n44.736\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\nF\n1828.443\n38.057\n\n\nR\n1772.368\n37.709\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nexemplar\n1921.894\n40.787\n\n\nnovel\n1678.954\n34.340\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\n500\nF\n1871.455\n64.744\n\n\n500\nR\n1740.414\n58.445\n\n\n1000\nF\n1787.203\n72.352\n\n\n1000\nR\n1769.701\n70.859\n\n\n2000\nF\n1826.308\n60.361\n\n\n2000\nR\n1806.996\n66.146\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nexemplar\n1918.958\n64.110\n\n\n500\nnovel\n1692.910\n58.709\n\n\n1000\nexemplar\n1909.325\n80.657\n\n\n1000\nnovel\n1647.475\n60.378\n\n\n2000\nexemplar\n1937.360\n66.291\n\n\n2000\nnovel\n1696.332\n59.484\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nF\nexemplar\n1996.547\n62.874\n\n\nF\nnovel\n1671.048\n43.255\n\n\nR\nexemplar\n1852.131\n52.539\n\n\nR\nnovel\n1687.363\n53.944\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nF\nexemplar\n2071.863\n106.270\n\n\n500\nF\nnovel\n1681.533\n73.306\n\n\n500\nR\nexemplar\n1774.055\n72.418\n\n\n500\nR\nnovel\n1704.916\n92.830\n\n\n1000\nF\nexemplar\n1940.675\n125.193\n\n\n1000\nF\nnovel\n1637.385\n72.791\n\n\n1000\nR\nexemplar\n1879.082\n102.911\n\n\n1000\nR\nnovel\n1657.686\n96.823\n\n\n2000\nF\nexemplar\n1976.841\n91.850\n\n\n2000\nF\nnovel\n1692.689\n78.484\n\n\n2000\nR\nexemplar\n1902.117\n95.137\n\n\n2000\nR\nnovel\n1700.436\n90.730"
  },
  {
    "objectID": "E3_DF_pairs.html#comparisons-1",
    "href": "E3_DF_pairs.html#comparisons-1",
    "title": "E3_DF_paired",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nRT$simple$DF_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nRT$simple$test_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "E3_DF_pairs.html#write-up-1",
    "href": "E3_DF_pairs.html#write-up-1",
    "title": "E3_DF_paired",
    "section": "Write-up",
    "text": "Write-up\nMean reaction times on correct trials for each subject in each condition were submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For brevity we report only the significant effects. The full analysis is contained in supplementary materials.\nThe main effect of lure type was significant, \\(F(1, 16) = 26.28\\), \\(\\mathit{MSE} = 114,329.02\\), \\(p < .001\\), \\(\\hat{\\eta}^2_G = .042\\). Mean reaction times were faster in the novel lure condition (M = 1678.954, SEM = 34.34) than exemplar lure condition (M = 1921.894, SEM = 40.787).\nThe remaining main effects and interactions were not significant."
  },
  {
    "objectID": "E3_DF_pairs.html#save-environment",
    "href": "E3_DF_pairs.html#save-environment",
    "title": "E3_DF_paired",
    "section": "save environment",
    "text": "save environment\n\nsave.image(\"data/E3/E3_data_write_up.RData\")"
  }
]